deepdive {
    
  db.default: {
    driver: "org.postgresql.Driver"
    url: "jdbc:postgresql://"${PGHOST}":"${PGPORT}"/"${PGDATABASE}
    user: ${PGUSER}
    password: ${PGPASSWORD}
    dbname: ${PGDATABASE}
    host: ${PGHOST}
    port: ${PGPORT}
  }

  calibration: {
    holdout_fraction: 0.5
  }

  schema.variables:{
    genegene.is_correct: Boolean
  }

  extraction.extractors: {

    ext_cleanup_genegene {
      sql: """
        DELETE FROM genegene;
      """
      style: "sql_extractor"
    }

    # Loads gene mentions and their features into the database
    load_genegene_relations:{
        style: "tsv_extractor"
        output_relation: "genegene"
        input: "SELECT document from documents where docid <> '15113120.html.txt.nlp' limit 300000"  
        udf: ${APP_HOME}"/py/gene_relations.py"
        #udf: util/extractor_input_writer.py /tmp/dd-sample-load_genegene_relations.txt
        parallelism: ${PARALLELISM}
        input_batch_size: 6000
        dependencies: ["ext_cleanup_genegene"]
    }

    load_docids:{
        before: ${APP_HOME}"/prepare_docids_data.sh" #truncate table name;
        output_relation: "docids"
        input: "SELECT 0"
        udf: ${APP_HOME}"/py/load_docids_pmc.py"
        dependencies: ["ext_cleanup"]
    }

    # Loads each documents into the database
    load_docs:{
        style: "tsv_extractor"
        before: ${APP_HOME}"/prepare_documents_data.sh" #truncate table name;
        output_relation: "documents"
        input: "SELECT distinct docid, folder FROM docids limit 500000" # limit 350000"
        udf: ${APP_HOME}"/py/load_docs.py"
        #output_batch_size: 25
        parallelism: ${PARALLELISM}
        input_batch_size: 5000
        output_batch_size: 5000
        dependencies: ["load_docids"]
    }

    # Loads each sentence into the database
    load_sents:{
        before: ${APP_HOME}"/prepare_sentences_data.sh" #truncate table name;
        output_relation: "sentences"
        input: "SELECT * from documents limit 400000"
        udf: ${APP_HOME}"/py/load_sents.py"
        parallelism: ${PARALLELISM}
        input_batch_size: 100
        output_batch_size: 5000
        dependencies: ["load_docs"]
    }

  }

  
  inference.batch_size = 500000

  inference.factors: {


    genegene_lr: {
      input_query: """
        SELECT genegene.docid as "distribute.distribute",
               genegene.id as "genegene.id", 
               genegene.is_correct as "genegene.is_correct",
               unnest(genegene.features) as "genegene.feature"
        FROM genegene
      """
      function: "Imply(genegene.is_correct)"
      weight: "?(genegene.feature)"
    }

    genegene_factors_crf.input_query: """
      select *
      from
        (select m1.id as "genegene.g1.id", m2.id as "genegene.g2.id", m1.is_correct as "genegene.g1.is_correct", m2.is_correct as "genegene.g2.is_correct",
          row_number() over (partition by m1.mid1) as rn
        from genegene m1, genegene m2
        where ((m1.mention1 = m2.mention1 and m1.mention2 = m2.mention2) or (m1.mention1 = m2.mention2 and m1.mention2 = m2.mention1)) and m1.docid = m2.docid and m1.mid1 < m2.mid1 and m1.mid2 < m2.mid1) mcrf
      where mcrf.rn = 1"""
    genegene_factors_crf.function: "Equal(genegene.g1.is_correct, genegene.g2.is_correct)"
    genegene_factors_crf.weight: "1" #try ? , 0.5



  }


  #pipeline.run: "onlygenegene"
  pipeline.pipelines.onlygenegene: [ "ext_cleanup_genegene", "load_genegene_relations", "genegene_lr", "genegene_factors_crf"]  

  #pipeline.run: "onlydocs"
  pipeline.pipelines.onlydocs: ["load_docids", "load_docs", "load_sents"]
   
  sampler.sampler_cmd: "util/sampler-dw-linux gibbs"
  sampler.sampler_args: "-l 1000 -s 1 -i 1000 --diminish 0.95 --alpha 0.001"

  sampler.java_args = "-Xmx"${MEMORY}


}
